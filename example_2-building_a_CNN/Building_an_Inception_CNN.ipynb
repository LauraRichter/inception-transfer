{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Inception CNN\n",
    "\n",
    "This is a copied and modified version of this great tutorial, [Inception models: explained and implimented](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, preprocessing and miscellaneous functions\n",
    "\n",
    "Following the Inception models tutorial, we'll use the csv-ified version of the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.loadtxt('mnist_train.csv', delimiter=',')\n",
    "test_set = np.loadtxt('mnist_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number image, the data is a row of:\n",
    "\n",
    "> number label (actual number value of the image), unravelled image data \n",
    "\n",
    "Let's print out the data shape and first few rows to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "(60000, 785)\n",
      "\n",
      "Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 4.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 9.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Data shape:')\n",
    "print(train_set.shape)\n",
    "\n",
    "print('\\nData:')\n",
    "train_set[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded label example:\n",
      "7.0 [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# extract the labels\n",
    "train_label_values = train_set[:, 0]\n",
    "test_label_values = test_set[:, 0]\n",
    "\n",
    "# one hot encode the labels\n",
    "#   in one hot encoding, we take our class labels and turn them into a vector of 1s and 0s, where each \n",
    "#   class is one element position of the vector, which is set to 1/True if the label is of that class\n",
    "train_labels = (np.arange(10) == train_label_values[:,None]).astype(np.float32)\n",
    "test_labels = (np.arange(10) == test_label_values[:,None]).astype(np.float32)\n",
    "\n",
    "print('One hot encoded label example:')\n",
    "print(test_label_values[0], test_labels[0])\n",
    "# the vector position 7 is assigned to class 7 in this case\n",
    "# so the vector is 1 there and zero elsewhere for this data element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# now extract the image data, by leaving out the first label column \n",
    "#   also convert to \n",
    "trainX = train_set[:, 1:]\n",
    "testX = test_set[:, 1:]\n",
    "\n",
    "print('data shape:')\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:\n",
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# now reshape each row into the image shape\n",
    "trainX = trainX.reshape(len(trainX),28,28,1)\n",
    "testX = testX.reshape(len(testX),28,28,1)\n",
    "\n",
    "print('data shape:')\n",
    "print(trainX.shape)\n",
    "# Note that we add the extra 1 dimension because these networks case input images with multiple colour panes\n",
    "# So the final dimension could be 3 for RGB, for example.\n",
    "# In this example our data is just black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the final 500 images in the list to be a validation set, and use the rest for training.\n",
    "n_validation = 500\n",
    "n_total = len(trainX)\n",
    "n_train = n_total - n_validation\n",
    "\n",
    "valX = trainX[n_train:]\n",
    "val_lb = train_labels[n_train:]\n",
    "\n",
    "trainX = trainX[0:n_train]\n",
    "train_lb = train_labels[0:n_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f690b7af7b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiAowg8ERfiBoP4fj3E4+31T1wcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f690b7d3470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check an image!\n",
    "example_image = trainX[300, :, :, 0]\n",
    "plt.imshow(example_image, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the Inception models tutorial's cleverness:\n",
    "#   need to batch the test data because running low on memory\n",
    "class test_batchs:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.batch_index = 0\n",
    "    def nextBatch(self,batch_size):\n",
    "        if (batch_size+self.batch_index) > self.data.shape[0]:\n",
    "            print(\"batch sized is messed up\")\n",
    "        batch = self.data[self.batch_index:(self.batch_index+batch_size),:,:,:]\n",
    "        self.batch_index= self.batch_index+batch_size\n",
    "        return batch\n",
    "\n",
    "# set the test batchsize\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the Inception models tutorial:\n",
    "#   returns accuracy of model\n",
    "def accuracy(target, predictions):\n",
    "    return(100.0*np.sum(np.argmax(target,1) == np.argmax(predictions,1))/target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the ourput model in our current working directory\n",
    "file_path = os.getcwd() + '/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Google Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "map1 = 32\n",
    "map2 = 64\n",
    "num_fc1 = 700 #1028\n",
    "num_fc2 = 10\n",
    "reduce1x1 = 16\n",
    "dropout = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # create tensorflow place holder tensors for the input and output data our model will use\n",
    "    \n",
    "    # training data and labels\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size,28,28,1))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(batch_size,10))\n",
    "    \n",
    "    # validation data\n",
    "    tf_valX = tf.placeholder(tf.float32, shape=(len(valX),28,28,1))\n",
    "    \n",
    "    # test data (for a batch at a time)\n",
    "    tf_testX=tf.placeholder(tf.float32, shape=(test_batch_size,28,28,1))\n",
    "    \n",
    "    # single element test data\n",
    "    tf_singleX=tf.placeholder(tf.float32, shape=(1,28,28,1))\n",
    "    \n",
    "    def createWeight(size,Name):\n",
    "        return tf.Variable(tf.truncated_normal(size, stddev=0.1),\n",
    "                          name=Name)\n",
    "    \n",
    "    def createBias(size,Name):\n",
    "        return tf.Variable(tf.constant(0.1,shape=size),\n",
    "                          name=Name)\n",
    "    \n",
    "    def conv2d_s1(x,W):\n",
    "        return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    def max_pool_3x3_s1(x):\n",
    "        return tf.nn.max_pool(x,ksize=[1,3,3,1],\n",
    "                             strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Inception Module1\n",
    "    #\n",
    "    # follows input\n",
    "    W_conv1_1x1_1 = createWeight([1,1,1,map1],'W_conv1_1x1_1')\n",
    "    b_conv1_1x1_1 = createWeight([map1],'b_conv1_1x1_1')\n",
    "    \n",
    "    # follows input\n",
    "    W_conv1_1x1_2 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_2')\n",
    "    b_conv1_1x1_2 = createWeight([reduce1x1],'b_conv1_1x1_2')\n",
    "    \n",
    "    # follows input\n",
    "    W_conv1_1x1_3 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_3')\n",
    "    b_conv1_1x1_3 = createWeight([reduce1x1],'b_conv1_1x1_3')\n",
    "    \n",
    "    # follows 1x1_2\n",
    "    W_conv1_3x3 = createWeight([3,3,reduce1x1,map1],'W_conv1_3x3')\n",
    "    b_conv1_3x3 = createWeight([map1],'b_conv1_3x3')\n",
    "    \n",
    "    # follows 1x1_3\n",
    "    W_conv1_5x5 = createWeight([5,5,reduce1x1,map1],'W_conv1_5x5')\n",
    "    b_conv1_5x5 = createBias([map1],'b_conv1_5x5')\n",
    "    \n",
    "    # follows max pooling\n",
    "    W_conv1_1x1_4= createWeight([1,1,1,map1],'W_conv1_1x1_4')\n",
    "    b_conv1_1x1_4= createWeight([map1],'b_conv1_1x1_4')\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Inception Module2\n",
    "    #\n",
    "    # follows inception1\n",
    "    W_conv2_1x1_1 = createWeight([1,1,4*map1,map2],'W_conv2_1x1_1')\n",
    "    b_conv2_1x1_1 = createWeight([map2],'b_conv2_1x1_1')\n",
    "    \n",
    "    # follows inception1\n",
    "    W_conv2_1x1_2 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_2')\n",
    "    b_conv2_1x1_2 = createWeight([reduce1x1],'b_conv2_1x1_2')\n",
    "    \n",
    "    # follows inception1\n",
    "    W_conv2_1x1_3 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_3')\n",
    "    b_conv2_1x1_3 = createWeight([reduce1x1],'b_conv2_1x1_3')\n",
    "    \n",
    "    # follows 1x1_2\n",
    "    W_conv2_3x3 = createWeight([3,3,reduce1x1,map2],'W_conv2_3x3')\n",
    "    b_conv2_3x3 = createWeight([map2],'b_conv2_3x3')\n",
    "    \n",
    "    # follows 1x1_3\n",
    "    W_conv2_5x5 = createWeight([5,5,reduce1x1,map2],'W_conv2_5x5')\n",
    "    b_conv2_5x5 = createBias([map2],'b_conv2_5x5')\n",
    "    \n",
    "    # follows max pooling\n",
    "    W_conv2_1x1_4= createWeight([1,1,4*map1,map2],'W_conv2_1x1_4')\n",
    "    b_conv2_1x1_4= createWeight([map2],'b_conv2_1x1_4')\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Fully connected layers\n",
    "    # since padding is same, the feature map with there will be 4 28*28*map2\n",
    "    W_fc1 = createWeight([28*28*(4*map2),num_fc1],'W_fc1')\n",
    "    b_fc1 = createBias([num_fc1],'b_fc1')\n",
    "    \n",
    "    W_fc2 = createWeight([num_fc1,num_fc2],'W_fc2')\n",
    "    b_fc2 = createBias([num_fc2],'b_fc2')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Stitch model together\n",
    "\n",
    "    def model(x,train=True):\n",
    "        # Inception Module 1\n",
    "        conv1_1x1_1 = conv2d_s1(x,W_conv1_1x1_1)+b_conv1_1x1_1\n",
    "        conv1_1x1_2 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_2)+b_conv1_1x1_2)\n",
    "        conv1_1x1_3 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_3)+b_conv1_1x1_3)\n",
    "        conv1_3x3 = conv2d_s1(conv1_1x1_2,W_conv1_3x3)+b_conv1_3x3\n",
    "        conv1_5x5 = conv2d_s1(conv1_1x1_3,W_conv1_5x5)+b_conv1_5x5\n",
    "        maxpool1 = max_pool_3x3_s1(x)\n",
    "        conv1_1x1_4 = conv2d_s1(maxpool1,W_conv1_1x1_4)+b_conv1_1x1_4\n",
    "        \n",
    "        # concatenate all the feature maps and hit them with a relu\n",
    "        #   (this line edited from original tutorial -- presumably a version change issue)\n",
    "        inception1 = tf.nn.relu(tf.concat([conv1_1x1_1,conv1_3x3,conv1_5x5,conv1_1x1_4], 3))\n",
    "\n",
    "        \n",
    "        # Inception Module 2\n",
    "        conv2_1x1_1 = conv2d_s1(inception1,W_conv2_1x1_1)+b_conv2_1x1_1\n",
    "        conv2_1x1_2 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_2)+b_conv2_1x1_2)\n",
    "        conv2_1x1_3 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_3)+b_conv2_1x1_3)\n",
    "        conv2_3x3 = conv2d_s1(conv2_1x1_2,W_conv2_3x3)+b_conv2_3x3\n",
    "        conv2_5x5 = conv2d_s1(conv2_1x1_3,W_conv2_5x5)+b_conv2_5x5\n",
    "        maxpool2 = max_pool_3x3_s1(inception1)\n",
    "        conv2_1x1_4 = conv2d_s1(maxpool2,W_conv2_1x1_4)+b_conv2_1x1_4\n",
    "        \n",
    "        # concatenate all the feature maps and hit them with a relu\n",
    "        #   (this line edited from original tutorial -- presumably a version change issue)\n",
    "        inception2 = tf.nn.relu(tf.concat([conv2_1x1_1,conv2_3x3,conv2_5x5,conv2_1x1_4], 3))\n",
    "\n",
    "        # flatten features for fully connected layer\n",
    "        inception2_flat = tf.reshape(inception2,[-1,28*28*4*map2])\n",
    "        \n",
    "        # Fully connected layers\n",
    "        if train:\n",
    "            h_fc1 =tf.nn.dropout(tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1),dropout)\n",
    "        else:\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "        return tf.matmul(h_fc1,W_fc2)+b_fc2\n",
    "\n",
    "    # (this line edited from original tutorial -- presumably a version change issue)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model(X), labels=y_))\n",
    "    opt = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    predictions_val = tf.nn.softmax(model(tf_valX, train=False))\n",
    "    predictions_test = tf.nn.softmax(model(tf_testX, train=False))\n",
    "    predictions_single = tf.nn.softmax(model(tf_singleX, train=False))\n",
    "    \n",
    "    # initialize variable\n",
    "    init = tf.global_variables_initializer() #tf.initialize_all_variables()\n",
    "    \n",
    "    # use to save variables so we can pick up later\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "step: 0\n",
      "validation accuracy: 16.6\n",
      " \n",
      "step: 100\n",
      "validation accuracy: 93.8\n",
      " \n",
      "step: 200\n",
      "validation accuracy: 93.6\n",
      " \n",
      "test accuracy: 91.07\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# num_steps is 20000 in the original tutorial -- we'll just use 200 to illustrate\n",
    "num_steps = 201\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "#initialize variables\n",
    "sess.run(init)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "#set use_previous=1 to use file_path model\n",
    "#set use_previous=0 to start model from scratch\n",
    "use_previous = 0\n",
    "\n",
    "#use the previous model or don't and initialize variables\n",
    "if use_previous:\n",
    "    saver.restore(sess,file_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "# Train\n",
    "for s in range(num_steps):\n",
    "    offset = (s*batch_size) % (len(trainX)-batch_size)\n",
    "    batch_x,batch_y = trainX[offset:(offset+batch_size),:],train_lb[offset:(offset+batch_size),:]\n",
    "    feed_dict = {X : batch_x, y_ : batch_y}\n",
    "    _,loss_value = sess.run([opt,loss],feed_dict=feed_dict)\n",
    "    if s%100 == 0:\n",
    "        feed_dict = {tf_valX : valX}\n",
    "        preds=sess.run(predictions_val,feed_dict=feed_dict)\n",
    "        \n",
    "        print(\"step: \"+str(s))\n",
    "        print(\"validation accuracy: \"+str(accuracy(val_lb,preds)))\n",
    "        print(\" \")\n",
    "        \n",
    "    # get test accuracy and save model\n",
    "    if s == (num_steps-1):\n",
    "        # create an array to store the outputs for the test\n",
    "        result = np.array([]).reshape(0,10)\n",
    "\n",
    "        # use the batches class\n",
    "        batch_testX=test_batchs(testX)\n",
    "\n",
    "        for i in range(len(testX)//test_batch_size):\n",
    "            feed_dict = {tf_testX : batch_testX.nextBatch(test_batch_size)}\n",
    "            preds = sess.run(predictions_test, feed_dict=feed_dict)\n",
    "            result = np.concatenate((result,preds),axis=0)\n",
    "        \n",
    "        print(\"test accuracy: \"+str(accuracy(test_labels,result)))\n",
    "        \n",
    "        save_path = saver.save(sess,file_path)\n",
    "        print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Test on a single example image, just to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result:\n",
      "[[0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6945450048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6944c89d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image = trainX[204, :, :, 0]\n",
    "feed_dict = {tf_singleX : example_image[np.newaxis, :, :, np.newaxis]}\n",
    "prediction = sess.run(predictions_single, feed_dict=feed_dict)\n",
    "\n",
    "print('Predicted result:')\n",
    "print(prediction.astype(int))\n",
    "plt.imshow(example_image, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
